\part{Dataset Labelling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dataset Labelling                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation and background}
This project is intimately linked with the neural networks project, but since a lot of time has been dedicated towards it, it merits a section for itself. This project is aimed at tackling the issue of lack of data, by providing a mechanism to get more data easily. The current bottleneck in aquiring more data is labelling the data, it's conceivably easy to set up a system to video hand washing, but to label it requires a bespoke system. This project is part of a broader initiative at Surewash to reevaluate the core computer vision system, and this project is part of the new testing framework. Surewash is collaborating with a group of researchers in Trinity, and the idea is to pool resources to develop reliable tools for marking up data for a computer system, as well as tools for evaluating the performance of the computer system.

Since this is a collaboration, the specification of this project is designed to meet the requirements of both Surewash, as well as the researchers at Trinity. The broad definition therefore of this project was something that could read in the data from an Intel Realsense camera, mark individual frames or segments of video as being part of $n \in \mathbb{N}$ classes, as well as marking the region of interest, hereby ROI. There also had to be a way of comparing the markup between differant people's markup. There were no constraints on the implementation of the project, although it is desirable in my opinion to make a cross-platform solution.

This project was assigned to both myself, as well as my fellow intern Gaurav. Since we're both somewhat familiar with Python and web development, we decided to use those tools to develop the solution. We agreed that I was more familiar with Python, and OS level scripting, so I mainly focused my efforts on server-side development, writing the programme for comparing marked up data, and any other miscellaneous scripts that handled file operations. Gaurav directed most of his attention towards writing the client-side code, such as the user interface.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Overview                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview Of Programme}
The process of labelling the data is divided into three distinct stages, first preprocessing to prepare the data for labelling, then the actual labelling of the data, and then comparison of the labelled data between differant people.
    \subsection{Preprocessing}
    The data is captured on an Intel Realsense camera, which saves the data in ROS-bag format which is an uncompressed video \cite{intelrosbag}. This needs to be converted into an ordinary video format (such as MPEG-4 \cite{wiegand2003overview}). There is a tool within the library of the Realsense camera which converts to still images, so another tool called FFMPEG \cite{ffmpeg} is used to convert to MPEG-4. This entire process is done with in a bash script, so the end user only needs to provide the ROS-bag file as an argument to the script and it will output a video that can be used for labelling.
    \subsection{Labelling}
    This programme is conceptually divided into two parts, the data is labelled in a GUI web app, and there is another backend programme that interfaces with the GUI web app. In the GUI web app, the user can play the video (or press a button to advance one frame at a time), and a series of buttons can be pressed while looking at the video to mark it as belonging to a particular class, see Figure \ref{fig:markupscreenshot}. One issue with this though is that there are many classes that could be present in a video, there are seven poses to be labeled, up to twenty sub poses with in those seven, and then those twnenty classes could be of high interest, or low interest, so up to forty in total. The problem thus is that no reasonable human operator would be able to watch a video and label it by pressing from a choice of forty buttons, this is where the backend programme is useful. For hand hygiene, the video could be labelled in levels, at the zeroth level, the user labels the video into seven classes, then the backend processes this markup and splits the video into seven non-overlapping sub-videos,  the user could label those sub-videos, and so on, until the desired level of granularity has been acheived.
    \begin{figure}[h]
        \centering
        \fbox{ \includegraphics[width=450px]{../img/markup_screenshot.png} }
        \caption{Screenshot of the labelling web app, all images are \copyright \space Glanta Ltd.}
        \label{fig:markupscreenshot}
    \end{figure}

    \subsection{Comparison} 
    It is likely that at least some errors will be made in the labelling step, therefore this step will assume that at least two differant people performed labelling on the same video, or at least the same person did the labelling twice. A programme was written that will compare two differant markup files, and it will report where there was disagreements between two markup files so that they can be looked at again.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Technical details                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Details}
    \subsection{Labelling - GUI}
    Since the app is a web app, the code is divided into two parts, the server code, and the client code. The layout of the output is as follows:

    \[
    \begin{bmatrix}
    x_{10} & y_{10} & x_{20} & y_{20} & c_{0} \\
    x_{11} & y_{11} & x_{21} & y_{21} & c_{1} \\
    x_{12} & y_{12} & x_{22} & y_{22} & c_{2} \\
    x_{13} & y_{13} & x_{23} & y_{23} & c_{3} \\
    \vdots & \vdots & \vdots & \vdots & \vdots\\
    x_{1n} & y_{1n} & x_{2n} & y_{2n} & c_{n} 
    \end{bmatrix}
    \]
    \begin{gather*}
    x, y \in \mathbb{R} [0, 1]\\
    c \in \mathbb{Z}
    \end{gather*}

    The $x$ and $y$ values represent coordinates describing an ROI box (top left, and bottom right), and the $c$ value represents the class (the classes must be encoded in integer form). Each row represents one frame. By default, all values are set to -1 to denote that they have either not been labelled, or are 'ambiguous'. The coordinates represent the ROI in a proportional system, so if the image is 350 pixels wide, and a $y$ coordinate was $0.22$, then that would refer to the 77th pixel. The coordinates are represented proportionally because the original video is scaled from the original BAG file, and since the ROI box does not have to be precisely correct, this system is acceptable in my opinion. In the markup file, the $x$ and $y$ values are represented by floating point values, and the class values are signed integers.

        \subsubsection{Server Code}
        This is primarily a Flask app \cite{flaskpocoo}. Flask is a Python web framework, similar in concept to Django but without the model layer, and web security features, which are not necesasary for this application. It is thus easier to write an application in Flask as opposed to Django. The server code is divided into two main areas, the view, and the template. The server acts 'dumb', its main task apart from serving the client application initially is to take the marked up data and save it, it does not do any processing with the data itself. The view exposes a HTTP GET, and HTTP POST method for downloading, and uploading the markup data.

        \subsubsection{Client Code}
        All of the logic for marking up the data happens in the client code, and since this is a web application, all of this code is written in JavaScript. The results are stored in an array of arrays. Each entry in the parent array corresponds to a frame, and each array in that corresponds to the coordinates, and class for that frame. Every time the video is played, paused, its frame put forward, or backward, it triggers a method to push the array back to the server using a Jquery AJAX method.

            \paragraph{Classes}
            The class is marked simply by pressing a button for what the class is for the currently displayed frame, it finds the buttons by looking in an appropriate directory within the app folder. The currently displayed frame is obtained with the following function:
                \begin{lstlisting}[style=JSStyle]
function return_current_frame() {
    var curr_frame = Math.floor(theVideo.currentTime*frameRate);
    if(curr_frame >= frame_count) {
        curr_frame = frame_count - 1;
    }
    return curr_frame;
}\end{lstlisting} 
            Since HTML5 video does not expose a way to get the frame number directly, it has to be obtained by multiplying the current time by the frame rate. Since there also is no way of determening the frame rate directly, a server-side HTTP GET method is used which uses ffprobe to determine the frame rate and push that to the client side using a Jquery AJAX method.

            \paragraph{ROI}
            Finding a way to mark ROI from a web browser is more difficult, since there weren't any out of the box solutions that existedas far as I could tell. The solution involved using a Javascript library called Cropper.js \cite{cropperjs}. This library is designed to load an image in for marking an area to crop. It was adapted to this solution by overlaying it on the video, and every time the video frame changed, the current cropped coordinates were obtained and saved to the results matrix. The method of overlaying led to issues with the coordinates that it outputted did not correspond to the video resolution due to the nature of this solution. The $x$ coordinates ranged from 0, to the width of the HTML5 canvas element, which means that this can be converted to proportional coordinates (mentioned above) easily. The $y$ coordinates on the other hand presented more issues, since they ranged from approximately $[-9.44, 159.31]$, and I could not source a reasonable explanation as to why this was the case. This quirk was consistent across browers and operating systems, so the $y$ coordinates were converted to proportional coordinates by fitting them to a line with $y=mx+c$ using the coordinates $(-9.44,0)$ and $(159.31,1)$. To prevent edge cases straying above 1, or below zero, the output values were clamped between these two values. The final code to do the conversion looks like the following:

            \begin{lstlisting}[style=JSStyle]
function get_proportional_coordinates(cropper_instance, x1_func, y1_func, x2_func, y2_func) {
    var curr_width = cropper_instance.getCanvasData().naturalWidth;
    var y1_proportional = (y1_func * (4/675)) + (944/16875);
    var y2_proportional = (y2_func * (4/675)) + (944/16875);
    var x1_proportional = x1_func / curr_width;
    var x2_proportional = x2_func / curr_width;

    if      (y1_proportional > 1)   y1_proportional = 1;
    else if (y1_proportional < 0)   y1_proportional = 0;

    if      (y2_proportional > 1)   y2_proportional = 1;
    else if (y2_proportional < 0)   y2_proportional = 0;

    if      (x1_proportional > 1)   x1_proportional = 1;
    else if (x1_proportional < 0)   x1_proportional = 0;

    if      (x2_proportional > 1)   x2_proportional = 1;
    else if (x2_proportional < 0)   x2_proportional = 0;
    
    return {
        x1_proportional: x1_proportional,
        y1_proportional: y1_proportional,
        x2_proportional: x2_proportional,
        y2_proportional: y2_proportional,
    }
}\end{lstlisting} 

    \subsection{Labelling - Backend}
    The backend for the GUI is a seperate application and process to the GUI. Using the Python Subprocess module, it launches the GUI app as a seperate process and communicates the markup using a pipeline. Before launching the GUI app, the backend moves the relevant classes to be marked into the GUI app directory, as well as a video, then launches the app. The levels of classes are layed out in a heirarchal directory, with the name of the image being the name of the class. The core to the backend programme is a recursive function that moves through this heirarchal directory.

    \paragraph{Labelling subsections of video}
    A core reason for the existence of this backend is that it allows a user to label the video in increasing levels of detail, a core issue that needs to be solved though is how to segment the labelled video. In the context of hand hygiene, when the user has labelled the video into seven poses, and is now about to add the sub labels, it would not be intuitive to have the watch the entire video seven times again for this process, and so it needs to be cropped. The backend cannot make any assumptions however about how that markup might look like, take Figure \ref{fig:labelsample} as an example, there are some sections that have been marked as ambiguous (and thus do not need any further labelling), not all classes appear in this video, and some sections are not continuous.

    \begin{figure}[h]
        \centering
        \includegraphics[width=450px]{../img/sample_label.png}
        \caption{A sample markup for a video}
        \label{fig:labelsample}
    \end{figure}

    The video is segmented by first binning the frame numbers into a python dictionary, where the key is the class, and the values for each key are the frame numbers. These values are sorted, and then joined into segments of continuous video. For each sub level, the video is then cropped and concatenated according to these segments. A sample segment might look like the following:
    \begin{lstlisting}[style=PythonStyle]
# The ROI has been ommitted from results for convenience
# Assume that there are only two classes to mark
# Each index in results corresponds to a frame number
results = [-1, -1, 1, 1, 1, 1, -1, 2, 2, 2, -1, 2, 2, 2, 2]
binned_results = {
    -1: [0, 1, 6, 10],
    1:  [2, 3, 4, 5],
    2:  [7, 8, 9, 11, 12, 13, 14]
}
segmented_results = {
    -1: [[0, 1], [6, 6], [10, 10]],
    1:  [[2, 5]],
    2:  [[7, 9], [11, 14]]
}\end{lstlisting}
    The ambiguous sections (-1) would be discarded, and two sub-videos would be made, one containing frames two to five, and one containing the concatenated frames of seven to nine, and eleven to fourteen. This process can be repeated for further levels of granulation.

    \paragraph{Saving progress mid-labelling}
    A feature request of the backend is that the user could stop in the middle of labelling a video, return at a later time, and come back to the same state. In Python, when a programme is signalled to stop mid-execution, a {\slshape KeyboardInterrupt} exception is thrown, and this exception is caught to save the state. In the normal execution of the recursive function, when it finishes, it returns an empty list, but if it ends with a {\slshape KeyboardInterrupt} being thrown, a list with the frame intervals is included, then the parent recursive calls appends the segmented results dictionary described above, as well as the list of frame intevals that it is labelling (the top level call will have a frame interbal of [[0, maximum\_frame]]). Extending the above example, if the subclasses of class 2 were being labelled and the user stopped the programme, the return value of the top level recursive function would be as follows

    \begin{lstlisting}[style=PythonStyle]
return_value = [[[7, 9], [11, 14]], { -1: [[0, 1], [6, 6], [10, 10]], 1:  [[2, 5]], 2:  [[7, 9], [11, 14]]}, [[0, 14]]]
# summary: return_value = [frame_interval, segmented_results, frame_interval]
# This is where there was one level of recursion, if there was two levels of recursion, it would look like this
# summary: return_value = [frame_interval, segmented_results, frame_interval, segmented_results, frame_interval]
# In general, the amount of levels of recursion is length((return_value - 1) / 2)\end{lstlisting}

    This return value, along with the results, and list of videos yet to be labelled are stored in file using Python's Pickle module (a built-in way of storing Python objects in File objects). When the user starts the programme again, if this Pickle file exists, it will reload the results, and call the recursive function again, and treat the return\_value variable as a stack. From the above example, it will know that the sub class of class 2 is nect to be labelled, because it can match [[7, 9], [11, 14]] with the key value 2 in the dictionary, and that class 1 has already been labelled because it calls itself in numerical order.

    \subsection{Comparison}
    % ************************************
    % Revise the wording of this paragraph
    % ************************************
    Two differant markups of the same video are compared with a Python script. The task is divided into two parts, one camparing the class markup, and one comparing the class markup.  To compare the class markup, each input is treated as a vector the process is first subtract one vector from the another, then from that output vector, produce another output vector where the output is one for non zero-values, and zero for zero values. The final result is an array of binary, where the index corresponds to the frame number. Since no markup is likely to be exactly the same, a threshold can be used whereby if the length of a sequence of ones is larger than that threshold, the programme can output that the two markups disagree at those points.
    
    A summary of the process can be seen in Figure \ref{fig:classprocessing}. Let $\pmb{a}$, $\pmb{b}$ be vectors ($n \times 1$ column matrix, where $n \in \mathbb{N}$ is the number of frames) denoting the marked up classes, where $\pmb{a}_i$, $\pmb{b}_i \in \mathbb{Z}$. The final binary array is $\pmb{d}$.
    \begin{figure}[h]
        \centering
        \begin{gather*}
            f(\pmb{x})=
        \begin{cases}%
        1      & \text{otherwise}\\
        0      & \text{if $x_i$ = 0}
        \end{cases} \\
        \pmb{c}=\pmb{a}-\pmb{b} \\
        \pmb{d}=f(\pmb{c})
        \end{gather*}
        \caption{}
        \label{fig:classprocessing}
    \end{figure}

    The process of comparing the ROI region is as follows. The distance between the two coordinate pairs for each markup is compared using $\sqrt[]{{(x_2-x_1)}^2+{(y_2-y_1)}^2}$ and both pairs are added together. This will produce a vector of positive real numbers, a binary version of this vector is produced where the value exceeds some threshold, and thus a vector like that of $\pmb{d}$ above is produced.

    A summary of the process is in Figure \ref{fig:roiprocessing}. Let $\pmb{A}$, $\pmb{B}$ be an $n \times 4$, matrix where $\pmb{A}_{ij}$, $\pmb{B}_{ij} \in \mathbb{R}^{n \times 4}$, $n \in \mathbb{N}$ is the number of frames, ${\pmb{A}}_{i1} \equiv x_1$, ${\pmb{A}}_{i2} \equiv y_1$, ${\pmb{A}}_{i3} \equiv x_2$, ${\pmb{A}}_{i4} \equiv y_2$, and the same for $\pmb{B}$. $T$ is the threshold.

    \begin{figure}[h]
        \centering
        
        \[
        \pmb{c}=
        \begin{bmatrix}
            \sqrt[]{{(B_{11} - A_{11})}^2 + {(B_{12} - A_{12})}^2} + \sqrt[]{{(B_{13} - A_{13})}^2 + {(B_{14} - A_{14})}^2} \\
            \sqrt[]{{(B_{21} - A_{21})}^2 + {(B_{22} - A_{22})}^2} + \sqrt[]{{(B_{23} - A_{23})}^2 + {(B_{24} - A_{24})}^2} \\
            \vdots \\
            \sqrt[]{{(B_{n1} - A_{n1})}^2 + {(B_{n2} - A_{n2})}^2} + \sqrt[]{{(B_{n3} - A_{n3})}^2 + {(B_{n4} - A_{n4})}^2} \\
        \end{bmatrix}
        \]
        \begin{gather*}
            f(\pmb{x})=
        \begin{cases}
        1      & \text{otherwise}\\
        0      & \text{if $x_i$ < T} \in \mathbb{R} 
        \end{cases} \\
        \pmb{d} = f(\pmb{c}) \\
        \end{gather*}
        \caption{}
        \label{fig:roiprocessing}
    \end{figure} 

    The optimal values for the thresholds mentioned are subjective, and depend on how one might want to balance time constraints with accuracy of the markup.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning Outcomes                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning outcomes}
    \subsection{Group Projects}
    I worked on this project with my fellow intern Guarav. This was the first proper project where I had to work as part of a group in a professional enviornment. I learned how to split the tasks of this project, based on the strengths and weaknesses of both of us, as well as the importance of reviewing each other's work.
    \subsubsection{Programming}
        \paragraph{Exception Handling}
        This was the first time where I made use of exception handling. I learned how putting code in try..cat
        \paragraph{Recursion}
        I learned how recursion can be used to parse tree data structures ds